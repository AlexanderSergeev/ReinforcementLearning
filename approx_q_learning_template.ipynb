{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a963bccd9100>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "\n",
    "def test_define_network(environment, net):\n",
    "    s = environment.reset()\n",
    "    assert tuple(net(Variable(torch.FloatTensor([s]*3))).size()) == (3, n_actions), \\\n",
    "        'please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]'\n",
    "\n",
    "    assert isinstance(list(net.modules())[-1], nn.Linear), \\\n",
    "        'please make sure you predict q-values without nonlinearity (ignore if you know what you are doing)'\n",
    "    assert isinstance(get_action(s), int), \\\n",
    "        'get_action(s) must return int, not %s. try int(action)' % (type(get_action(s)))\n",
    "\n",
    "    print('Test #1: define_network() & get_action() functions: OK!')\n",
    "\n",
    "\n",
    "def test_eps_greedy_strategy():\n",
    "    # Test epsilon-greedy exploration\n",
    "    for eps in [0., 0.1, 0.5, 1.0]:\n",
    "        state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "        best_action = state_frequencies.argmax()\n",
    "        assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
    "        for other_action in range(n_actions):\n",
    "            if other_action != best_action:\n",
    "                assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
    "        print('eps=%.1f tests passed' % eps)\n",
    "    print('Test #2: epsilon greedy exploration: OK!')\n",
    "\n",
    "\n",
    "def test_td_loss(environment, net):\n",
    "    s = environment.reset()\n",
    "    a = environment.action_space.sample()\n",
    "    next_s, r, done, _ = env.step(a)\n",
    "    loss = compute_td_loss([s], [a], [r], [next_s], [done], check_shapes=False)\n",
    "    loss.backward()\n",
    "\n",
    "    assert isinstance(loss, Variable) and tuple(loss.data.size()) == (1,), \\\n",
    "        'you must return scalar loss - mean over batch'\n",
    "    assert np.any(next(net.parameters()).grad.data.numpy() != 0), \\\n",
    "        'loss must be differentiable w.r.t. network weights'\n",
    "\n",
    "    print('Test #3: compute_td_loss() function: OK!')\n",
    "\n",
    "\n",
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" helper #1: take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot\n",
    "\n",
    "\n",
    "def where(cond, x_1, x_2):\n",
    "    \"\"\" helper #2: like np.where but in PyTorch. \"\"\"\n",
    "    return (cond * x_1) + ((1-cond) * x_2)\n",
    "\n",
    "\n",
    "# < YOUR CODE HERE >\n",
    "def define_network(state_dim, n_actions):\n",
    "    network = nn.Sequential()\n",
    "    return network\n",
    "\n",
    "\n",
    "# < YOUR CODE HERE >\n",
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with probability = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    state = None\n",
    "    q_values = None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# < YOUR CODE HERE >\n",
    "def compute_td_loss(states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False):\n",
    "    \"\"\" Compute td loss using torch operations only.\"\"\"\n",
    "    states = Variable(torch.FloatTensor(states))  # shape: [batch_size, state_size]\n",
    "    actions = Variable(torch.IntTensor(actions))  # shape: [batch_size]\n",
    "    rewards = Variable(torch.FloatTensor(rewards))  # shape: [batch_size]\n",
    "    next_states = Variable(torch.FloatTensor(next_states))  # shape: [batch_size, state_size]\n",
    "    is_done = Variable(torch.FloatTensor(is_done))  # shape: [batch_size]\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = None  # < YOUR CODE HERE >\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = torch.sum(predicted_qvalues.cpu() * to_one_hot(actions, n_actions), dim=1)\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = None  # < YOUR CODE HERE >\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = None  # < YOUR CODE HERE >\n",
    "\n",
    "    assert isinstance(next_state_values.data, torch.FloatTensor)\n",
    "\n",
    "    # compute 'target q-values' for loss\n",
    "    target_qvalues_for_actions = None  # < YOUR CODE HERE >\n",
    "\n",
    "    # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    target_qvalues_for_actions = where(is_done, rewards, target_qvalues_for_actions).cpu()\n",
    "\n",
    "    # Mean Squared Error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
    "            'make sure you predicted q-values for all actions in next state'\n",
    "        assert next_state_values.data.dim() == 1, \\\n",
    "            'make sure you computed V(s-prime) as maximum over just the actions axis and not all axes'\n",
    "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
    "            'there is something wrong with target q-values, they must be a vector'\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"Play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # a = <get_action_a> from agent # < YOUR CODE HERE >\n",
    "        a = None\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            loss = compute_td_loss([s], [a], [r], [next_s], [done])\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dump_logs = False\n",
    "    record_video = False\n",
    "    env = gym.make(\"CartPole-v0\").env\n",
    "    s = env.reset()\n",
    "    n_actions = env.action_space.n\n",
    "    state_dim = env.observation_space.shape\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    print('Actions number = %i , State example = %s ' % (n_actions, s))\n",
    "    print('State space upper bound: %s' % env.observation_space.high)\n",
    "    print('State space lower bound: %s' % env.observation_space.low)\n",
    "\n",
    "    # Complete define_network() & get_action() functions\n",
    "    network = define_network(state_dim, n_actions)\n",
    "\n",
    "    test_define_network(env, network)\n",
    "    test_eps_greedy_strategy()\n",
    "\n",
    "    # Complete compute_td_loss function\n",
    "    test_td_loss(env, network)\n",
    "\n",
    "    # Create Adam optimizer with lr=1e-4\n",
    "    opt = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "    epsilon = 0.4\n",
    "    max_epochs = 1000\n",
    "    if dump_logs:\n",
    "        log_path = './logs/{:%Y_%m_%d_%H_%M}'.format(datetime.datetime.now())\n",
    "        writer = SummaryWriter(log_path)\n",
    "\n",
    "    for i in range(max_epochs):\n",
    "        session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
    "        print('Epoch #{}\\tMean reward = {:.3f}\\tEpsilon = {:.3f}'.format(i, np.mean(session_rewards), epsilon))\n",
    "        if dump_logs:\n",
    "            writer.add_scalar('Mean Reward', np.mean(session_rewards), i)\n",
    "\n",
    "        # Code Epsilon decay <HERE>\n",
    "        # epsilon ?\n",
    "        assert epsilon >= 1e-4, 'Make sure epsilon is always nonzero during training'\n",
    "\n",
    "        if np.mean(session_rewards) > 300:\n",
    "            print('You Win!')\n",
    "            break\n",
    "    if record_video:\n",
    "        env = gym.wrappers.Monitor(gym.make('CartPole-v0').env, directory='videos', force=True)\n",
    "        sessions = [generate_session(epsilon=0, train=False) for _ in range(100)]\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
