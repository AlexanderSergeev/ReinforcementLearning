{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ac6bbde10aa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;31m# Create environment 'CartPole-v0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;31m# Compute number of actions for this environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_session(agent, t_max=10**5):\n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Choose action from policy\n",
    "        # You can use np.random.choice() func\n",
    "        # a = ?\n",
    "        a = None\n",
    "\n",
    "        # Do action `a` to obtain new_state, reward, is_done,\n",
    "        new_s, r, is_done = None, None, None\n",
    "\n",
    "        # Record state, action and add up reward to states, actions and total_reward accordingly.\n",
    "        # states\n",
    "        # actions\n",
    "        # total_reward\n",
    "\n",
    "        # Update s for new cycle iteration\n",
    "\n",
    "        if is_done:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward\n",
    "\n",
    "\n",
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order\n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\"\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = map(np.array, [states_batch, actions_batch, rewards_batch])\n",
    "\n",
    "    # Compute reward threshold\n",
    "    reward_threshold = None\n",
    "\n",
    "    # Compute elite states using reward threshold\n",
    "    elite_states = None\n",
    "\n",
    "    # Compute elite actions using reward threshold\n",
    "    elite_actions = None\n",
    "\n",
    "    elite_states, elite_actions = map(np.concatenate, [elite_states, elite_actions])\n",
    "\n",
    "    return elite_states, elite_actions\n",
    "\n",
    "\n",
    "def rl_approx_cross_entropy(nn_agent):\n",
    "    n_sessions = 100\n",
    "    percentile = 70\n",
    "    total_iterations = 100\n",
    "    log = []\n",
    "\n",
    "    for i in range(total_iterations):\n",
    "\n",
    "        # Generate n_sessions for further analysis.\n",
    "        sessions = None\n",
    "        states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
    "\n",
    "        # Select elite states & actions.\n",
    "        elite_states, elite_actions = None, None\n",
    "\n",
    "        # Update policy using elite_states, elite_actions.\n",
    "        # nn_agent\n",
    "\n",
    "        # Info for debugging\n",
    "        mean_reward = np.mean(rewards_batch)\n",
    "        threshold = np.percentile(rewards_batch, percentile)\n",
    "        log.append([mean_reward, threshold])\n",
    "\n",
    "        print('Iteration= %.3f, Mean Reward = %.3f, Threshold=%.3f' % (i, mean_reward, threshold))\n",
    "\n",
    "        if np.mean(rewards_batch) > 195:\n",
    "            print('You Win! :)')\n",
    "            break\n",
    "\n",
    "\n",
    "def test_rl_approx_cross_entropy(nn_agent):\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(1000):\n",
    "        # Choose action from nn_agent\n",
    "        # You can use np.random.choice() func\n",
    "        # a = ?\n",
    "        a = None\n",
    "\n",
    "        # Do action `a` to obtain new_state, reward, is_done,\n",
    "        new_s, r, is_done = None, None, None\n",
    "\n",
    "        if is_done:\n",
    "            break\n",
    "        else:\n",
    "            env.render()\n",
    "            time.sleep(0.07)\n",
    "            total_reward += r\n",
    "            # Update s for new cycle iteration\n",
    "\n",
    "    print('Reward of Test agent = %.3f' % total_reward)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create environment 'CartPole-v0'\n",
    "    env = None\n",
    "    s = env.reset()\n",
    "\n",
    "    # Compute number of actions for this environment\n",
    "    n_actions = None\n",
    "\n",
    "    print('Actions number = %i' % n_actions)\n",
    "\n",
    "    # Create neural network with 2 hidden layers of 10 & 10 neurons each & tanh activations\n",
    "    # use MLPClassifier from scikit-learn\n",
    "\n",
    "    agent = None\n",
    "\n",
    "    # Initialize agent to the dimension of state and amount of actions\n",
    "    agent.fit([s] * n_actions, range(n_actions))\n",
    "\n",
    "    # Train `deep` neural network to approximate cross entropy method\n",
    "    rl_approx_cross_entropy(agent)\n",
    "\n",
    "    # Test our NN and see how it performs\n",
    "    test_rl_approx_cross_entropy(agent)\n",
    "\n",
    "    # Close environment when everything is done\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
