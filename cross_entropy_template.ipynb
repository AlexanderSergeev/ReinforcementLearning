{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_policy_init(initial_policy):\n",
    "    assert type(initial_policy) in (np.ndarray, np.matrix)\n",
    "    assert np.allclose(initial_policy, 1. / n_actions)\n",
    "    assert np.allclose(np.sum(initial_policy, axis=1), 1)\n",
    "    print('Policy initialization: Ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_generate_session_func(generation_func):\n",
    "    s, a, r = generation_func(policy)\n",
    "    assert type(s) == type(a) == list\n",
    "    assert len(s) == len(a)\n",
    "    assert type(r) in [float, np.float]\n",
    "    print('Session generation function: Ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_update_policy_func(update_func):\n",
    "    elite_states, elite_actions = ([1, 2, 3, 4, 2, 0, 2, 3, 1], [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
    "    new_policy = update_func(elite_states, elite_actions, 5, 6)\n",
    "\n",
    "    assert np.isfinite(new_policy).all(), 'Your new policy contains NaNs or +-inf. Make sure you do not divide by zero.'\n",
    "    assert np.all(new_policy >= 0), 'Your new policy should not have negative action probabilities'\n",
    "    assert np.allclose(new_policy.sum(axis=-1), 1),  \\\n",
    "        'Your new policy should be a valid probability distribution over actions'\n",
    "\n",
    "    reference_answer = np.array([\n",
    "           [1.,  0.,  0.,  0.,  0.],\n",
    "           [0.5,  0.,  0.,  0.5,  0.],\n",
    "           [0.,  0.33333333,  0.66666667,  0.,  0.],\n",
    "           [0.,  0.,  0.,  0.5,  0.5]])\n",
    "    assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "    print('Update policy function: Ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_select_elites_func(select_elite_func):\n",
    "    states_batch = [[1, 2, 3], [4, 2, 0, 2], [3, 1]]\n",
    "    actions_batch = [[0, 2, 4], [3, 2, 0, 1], [3, 3]]\n",
    "    rewards_batch = [3, 4, 5]\n",
    "\n",
    "    test_result_0 = select_elite_func(states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "    test_result_40 = select_elite_func(states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "    test_result_90 = select_elite_func(states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "    test_result_100 = select_elite_func(states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "    assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1]) and \\\n",
    "        np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]), \\\n",
    "        'For percentile 0 you should return all states and actions in chronological order'\n",
    "\n",
    "    assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "        np.all(test_result_40[1] == [3, 2, 0, 1, 3, 3]), \\\n",
    "        'For percentile 30 you should only select states/actions from two first'\n",
    "\n",
    "    assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "        np.all(test_result_90[1] == [3, 3]), \\\n",
    "        'For percentile 90 you should only select states/actions from one game'\n",
    "\n",
    "    assert np.all(test_result_100[0] == [3, 1]) and \\\n",
    "        np.all(test_result_100[1] == [3, 3]), \\\n",
    "        'Please make sure you use >=, not >. Also double-check how you compute percentile.'\n",
    "    print('Select elites function : Ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(policy, t_max=10**5):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Choose action from policy\n",
    "        # You can use np.random.choice() func\n",
    "        # a = ?\n",
    "        a = np.random.choice(n_actions, p=policy[s])\n",
    "\n",
    "        # Do action `a` to obtain new_state, reward, is_done,\n",
    "        new_s, r, is_done, _ = env.step(a)\n",
    "\n",
    "        # Record state, action and add up reward to states, actions and total_reward accordingly.\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        # Update s for new cycle iteration\n",
    "        s = new_s\n",
    "        if is_done:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order\n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\"\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = map(np.array, [states_batch, actions_batch, rewards_batch])\n",
    "\n",
    "    # Compute reward threshold\n",
    "    reward_threshold = np.percentile(rewards_batch, q=percentile)\n",
    "\n",
    "    # Compute elite states using reward threshold\n",
    "    elite_states = states_batch[rewards_batch >= reward_threshold]\n",
    "\n",
    "    # Compute elite actions using reward threshold\n",
    "    elite_actions = actions_batch[rewards_batch >= reward_threshold]\n",
    "\n",
    "    elite_states, elite_actions = map(np.concatenate, [elite_states, elite_actions])\n",
    "\n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions, n_states, n_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "    new_policy = np.zeros([n_states, n_actions])\n",
    "\n",
    "    for el_state, el_action in zip(elite_states, elite_actions):\n",
    "        new_policy[el_state][el_action] += 1\n",
    "\n",
    "    sum_array = new_policy.sum(axis=1)\n",
    "\n",
    "    for s in range(0, n_states):\n",
    "        if sum_array[s] == 0:\n",
    "            new_policy[s] = np.full((1, n_actions), 1.0 / n_actions)\n",
    "        else:\n",
    "            new_policy[s] /= sum_array[s]\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_cross_entropy():\n",
    "    # Useful constants, all should be applied somewhere in your code\n",
    "    n_sessions = 200  # generate n_sessions for analysis\n",
    "    # чем больше n_sessions, тем намного медленнее спуск, но больше mean reward (+3 после 100 итераций и n_sessions = 500)\n",
    "    percentile = 20  # take this percentage of 'elite' states/actions\n",
    "    # чем меньше percentile, тем медленнее спуск и меньше mean reward (-161 после 100 итераций и percentile = 10)\n",
    "    alpha = 0.3  # alpha-blending for policy updates\n",
    "    # чем больше alpha, тем меньше mean reward (-4 после 100 итераций и alpha = 0.5)\n",
    "    # сейчас mean reward примерно -0.5 после 100 итераций\n",
    "    total_iterations = 100\n",
    "    # чем больше total_iterations, тем чаще положительный mean reward, ниже theshold\n",
    "    visualize = False\n",
    "    log = []\n",
    "\n",
    "    # Create random uniform policy\n",
    "    policy = np.full([n_states, n_actions], 1.0 / n_actions)\n",
    "    check_policy_init(policy)\n",
    "\n",
    "    if visualize:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=[10, 4])\n",
    "\n",
    "    for i in range(total_iterations):\n",
    "\n",
    "        # Generate n_sessions for further analysis.\n",
    "        sessions = [generate_session(policy) for _ in range(n_sessions)]\n",
    "\n",
    "        states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "\n",
    "        # Select elite states & actions.\n",
    "        elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
    "\n",
    "        # Update policy using elite_states, elite_actions.\n",
    "        new_policy = update_policy(elite_states, elite_actions, n_states, n_actions)\n",
    "\n",
    "        # Alpha blending of old & new policies for stability.\n",
    "        policy = alpha * new_policy + (1 - alpha) * policy\n",
    "\n",
    "        # Info for debugging\n",
    "        mean_reward = np.mean(rewards_batch)\n",
    "        threshold = np.percentile(rewards_batch, percentile)\n",
    "        log.append([mean_reward, threshold])\n",
    "\n",
    "        print('Iteration = %.0f, Mean Reward = %.3f, Threshold = %.3f' % (i, mean_reward, threshold))\n",
    "\n",
    "        # Visualize training\n",
    "        if visualize:\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(list(zip(*log))[0], label='Mean rewards', color='red')\n",
    "            plt.plot(list(zip(*log))[1], label='Reward thresholds', color='green')\n",
    "\n",
    "            if i == 0:\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(rewards_batch, range=[-990, +10], color='blue', label='Rewards distribution')\n",
    "            plt.vlines([np.percentile(rewards_batch, percentile)], [0], [100], label='Percentile', color='red')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "            plt.pause(0.1)\n",
    "            plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "States number = 500, Actions number = 6\n",
      "Policy initialization: Ok!\n",
      "Session generation function: Ok!\n",
      "Select elites function : Ok!\n",
      "Update policy function: Ok!\n",
      "Policy initialization: Ok!\n",
      "Iteration = 0, Mean Reward = -780.755, Threshold = -839.000\n",
      "Iteration = 1, Mean Reward = -773.370, Threshold = -830.000\n",
      "Iteration = 2, Mean Reward = -742.790, Threshold = -830.000\n",
      "Iteration = 3, Mean Reward = -743.320, Threshold = -804.800\n",
      "Iteration = 4, Mean Reward = -719.885, Threshold = -803.000\n",
      "Iteration = 5, Mean Reward = -704.940, Threshold = -794.000\n",
      "Iteration = 6, Mean Reward = -708.430, Threshold = -785.000\n",
      "Iteration = 7, Mean Reward = -703.175, Threshold = -785.000\n",
      "Iteration = 8, Mean Reward = -690.130, Threshold = -785.000\n",
      "Iteration = 9, Mean Reward = -680.200, Threshold = -767.000\n",
      "Iteration = 10, Mean Reward = -677.305, Threshold = -767.000\n",
      "Iteration = 11, Mean Reward = -659.430, Threshold = -749.400\n",
      "Iteration = 12, Mean Reward = -670.480, Threshold = -749.000\n",
      "Iteration = 13, Mean Reward = -647.740, Threshold = -749.000\n",
      "Iteration = 14, Mean Reward = -647.340, Threshold = -731.000\n",
      "Iteration = 15, Mean Reward = -609.705, Threshold = -722.000\n",
      "Iteration = 16, Mean Reward = -583.235, Threshold = -704.000\n",
      "Iteration = 17, Mean Reward = -589.150, Threshold = -695.000\n",
      "Iteration = 18, Mean Reward = -573.340, Threshold = -678.800\n",
      "Iteration = 19, Mean Reward = -559.220, Threshold = -677.000\n",
      "Iteration = 20, Mean Reward = -545.785, Threshold = -668.000\n",
      "Iteration = 21, Mean Reward = -542.585, Threshold = -660.800\n",
      "Iteration = 22, Mean Reward = -521.995, Threshold = -659.000\n",
      "Iteration = 23, Mean Reward = -527.725, Threshold = -650.000\n",
      "Iteration = 24, Mean Reward = -507.780, Threshold = -641.000\n",
      "Iteration = 25, Mean Reward = -486.150, Threshold = -632.000\n",
      "Iteration = 26, Mean Reward = -465.195, Threshold = -614.000\n",
      "Iteration = 27, Mean Reward = -474.130, Threshold = -606.800\n",
      "Iteration = 28, Mean Reward = -443.785, Threshold = -587.000\n",
      "Iteration = 29, Mean Reward = -451.185, Threshold = -596.000\n",
      "Iteration = 30, Mean Reward = -401.835, Threshold = -560.000\n",
      "Iteration = 31, Mean Reward = -397.045, Threshold = -561.000\n",
      "Iteration = 32, Mean Reward = -381.970, Threshold = -542.000\n",
      "Iteration = 33, Mean Reward = -366.555, Threshold = -516.800\n",
      "Iteration = 34, Mean Reward = -350.510, Threshold = -525.800\n",
      "Iteration = 35, Mean Reward = -344.460, Threshold = -498.200\n",
      "Iteration = 36, Mean Reward = -333.030, Threshold = -488.000\n",
      "Iteration = 37, Mean Reward = -315.240, Threshold = -474.200\n",
      "Iteration = 38, Mean Reward = -284.775, Threshold = -452.000\n",
      "Iteration = 39, Mean Reward = -263.565, Threshold = -432.400\n",
      "Iteration = 40, Mean Reward = -247.990, Threshold = -397.200\n",
      "Iteration = 41, Mean Reward = -245.940, Threshold = -389.800\n",
      "Iteration = 42, Mean Reward = -234.045, Threshold = -372.200\n",
      "Iteration = 43, Mean Reward = -203.955, Threshold = -339.000\n",
      "Iteration = 44, Mean Reward = -182.250, Threshold = -267.600\n",
      "Iteration = 45, Mean Reward = -197.840, Threshold = -302.000\n",
      "Iteration = 46, Mean Reward = -174.030, Threshold = -239.400\n",
      "Iteration = 47, Mean Reward = -175.870, Threshold = -234.200\n",
      "Iteration = 48, Mean Reward = -156.780, Threshold = -204.400\n",
      "Iteration = 49, Mean Reward = -145.560, Threshold = -184.200\n",
      "Iteration = 50, Mean Reward = -145.615, Threshold = -164.200\n",
      "Iteration = 51, Mean Reward = -129.695, Threshold = -177.200\n",
      "Iteration = 52, Mean Reward = -102.340, Threshold = -154.400\n",
      "Iteration = 53, Mean Reward = -94.055, Threshold = -143.400\n",
      "Iteration = 54, Mean Reward = -95.550, Threshold = -131.000\n",
      "Iteration = 55, Mean Reward = -76.265, Threshold = -118.400\n",
      "Iteration = 56, Mean Reward = -78.950, Threshold = -112.000\n",
      "Iteration = 57, Mean Reward = -62.840, Threshold = -93.400\n",
      "Iteration = 58, Mean Reward = -62.850, Threshold = -91.200\n",
      "Iteration = 59, Mean Reward = -52.780, Threshold = -79.200\n",
      "Iteration = 60, Mean Reward = -49.255, Threshold = -77.000\n",
      "Iteration = 61, Mean Reward = -43.990, Threshold = -65.000\n",
      "Iteration = 62, Mean Reward = -42.420, Threshold = -65.000\n",
      "Iteration = 63, Mean Reward = -37.535, Threshold = -59.200\n",
      "Iteration = 64, Mean Reward = -32.915, Threshold = -52.200\n",
      "Iteration = 65, Mean Reward = -34.870, Threshold = -55.200\n",
      "Iteration = 66, Mean Reward = -29.180, Threshold = -47.000\n",
      "Iteration = 67, Mean Reward = -26.230, Threshold = -38.000\n",
      "Iteration = 68, Mean Reward = -24.580, Threshold = -40.200\n",
      "Iteration = 69, Mean Reward = -20.320, Threshold = -35.200\n",
      "Iteration = 70, Mean Reward = -20.990, Threshold = -39.200\n",
      "Iteration = 71, Mean Reward = -16.945, Threshold = -31.000\n",
      "Iteration = 72, Mean Reward = -15.070, Threshold = -26.200\n",
      "Iteration = 73, Mean Reward = -15.965, Threshold = -27.000\n",
      "Iteration = 74, Mean Reward = -14.770, Threshold = -28.200\n",
      "Iteration = 75, Mean Reward = -15.615, Threshold = -30.200\n",
      "Iteration = 76, Mean Reward = -10.945, Threshold = -21.000\n",
      "Iteration = 77, Mean Reward = -12.520, Threshold = -22.000\n",
      "Iteration = 78, Mean Reward = -10.990, Threshold = -21.000\n",
      "Iteration = 79, Mean Reward = -9.970, Threshold = -21.200\n",
      "Iteration = 80, Mean Reward = -9.470, Threshold = -20.000\n",
      "Iteration = 81, Mean Reward = -8.750, Threshold = -17.200\n",
      "Iteration = 82, Mean Reward = -8.055, Threshold = -17.000\n",
      "Iteration = 83, Mean Reward = -7.945, Threshold = -16.400\n",
      "Iteration = 84, Mean Reward = -9.185, Threshold = -18.200\n",
      "Iteration = 85, Mean Reward = -8.950, Threshold = -17.000\n",
      "Iteration = 86, Mean Reward = -7.225, Threshold = -14.000\n",
      "Iteration = 87, Mean Reward = -4.070, Threshold = -13.000\n",
      "Iteration = 88, Mean Reward = -4.480, Threshold = -11.200\n",
      "Iteration = 89, Mean Reward = -5.210, Threshold = -13.000\n",
      "Iteration = 90, Mean Reward = -5.230, Threshold = -11.000\n",
      "Iteration = 91, Mean Reward = -3.975, Threshold = -13.000\n",
      "Iteration = 92, Mean Reward = -3.625, Threshold = -13.000\n",
      "Iteration = 93, Mean Reward = -6.215, Threshold = -14.400\n",
      "Iteration = 94, Mean Reward = -3.365, Threshold = -9.200\n",
      "Iteration = 95, Mean Reward = -2.470, Threshold = -8.400\n",
      "Iteration = 96, Mean Reward = -2.575, Threshold = -6.200\n",
      "Iteration = 97, Mean Reward = -4.390, Threshold = -10.200\n",
      "Iteration = 98, Mean Reward = -2.940, Threshold = -8.000\n",
      "Iteration = 99, Mean Reward = -3.715, Threshold = -8.000\n",
      "Iteration = 100, Mean Reward = -5.865, Threshold = -15.000\n",
      "Iteration = 101, Mean Reward = -5.580, Threshold = -14.400\n",
      "Iteration = 102, Mean Reward = -3.465, Threshold = -8.000\n",
      "Iteration = 103, Mean Reward = -3.050, Threshold = -11.000\n",
      "Iteration = 104, Mean Reward = -1.900, Threshold = -8.000\n",
      "Iteration = 105, Mean Reward = -2.325, Threshold = -9.000\n",
      "Iteration = 106, Mean Reward = -2.725, Threshold = -8.000\n",
      "Iteration = 107, Mean Reward = -3.065, Threshold = -7.000\n",
      "Iteration = 108, Mean Reward = -2.150, Threshold = -7.200\n",
      "Iteration = 109, Mean Reward = -3.305, Threshold = -7.200\n",
      "Iteration = 110, Mean Reward = -1.035, Threshold = -6.200\n",
      "Iteration = 111, Mean Reward = -1.845, Threshold = -7.200\n",
      "Iteration = 112, Mean Reward = -0.615, Threshold = -7.000\n",
      "Iteration = 113, Mean Reward = -1.485, Threshold = -8.000\n",
      "Iteration = 114, Mean Reward = 0.625, Threshold = -3.000\n",
      "Iteration = 115, Mean Reward = -0.375, Threshold = -7.000\n",
      "Iteration = 116, Mean Reward = 0.290, Threshold = -5.200\n",
      "Iteration = 117, Mean Reward = -0.715, Threshold = -8.000\n",
      "Iteration = 118, Mean Reward = -1.930, Threshold = -7.000\n",
      "Iteration = 119, Mean Reward = 1.010, Threshold = -5.000\n",
      "Iteration = 120, Mean Reward = -2.295, Threshold = -6.200\n",
      "Iteration = 121, Mean Reward = 1.190, Threshold = -5.000\n",
      "Iteration = 122, Mean Reward = -2.040, Threshold = -9.000\n",
      "Iteration = 123, Mean Reward = -0.465, Threshold = -6.000\n",
      "Iteration = 124, Mean Reward = -3.070, Threshold = -7.000\n",
      "Iteration = 125, Mean Reward = 2.015, Threshold = -3.000\n",
      "Iteration = 126, Mean Reward = -2.070, Threshold = -8.000\n",
      "Iteration = 127, Mean Reward = -1.085, Threshold = -6.000\n",
      "Iteration = 128, Mean Reward = -0.355, Threshold = -5.000\n",
      "Iteration = 129, Mean Reward = 2.085, Threshold = -1.000\n",
      "Iteration = 130, Mean Reward = -0.475, Threshold = -6.000\n",
      "Iteration = 131, Mean Reward = -2.385, Threshold = -8.000\n",
      "Iteration = 132, Mean Reward = -2.105, Threshold = -5.000\n",
      "Iteration = 133, Mean Reward = -3.845, Threshold = -8.200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 134, Mean Reward = -1.210, Threshold = -8.200\n",
      "Iteration = 135, Mean Reward = -0.935, Threshold = -6.000\n",
      "Iteration = 136, Mean Reward = -2.865, Threshold = -7.200\n",
      "Iteration = 137, Mean Reward = -1.670, Threshold = -7.000\n",
      "Iteration = 138, Mean Reward = 0.940, Threshold = -2.000\n",
      "Iteration = 139, Mean Reward = -0.220, Threshold = -5.000\n",
      "Iteration = 140, Mean Reward = 0.105, Threshold = -8.000\n",
      "Iteration = 141, Mean Reward = 1.070, Threshold = -4.000\n",
      "Iteration = 142, Mean Reward = 0.825, Threshold = -3.200\n",
      "Iteration = 143, Mean Reward = -3.380, Threshold = -8.400\n",
      "Iteration = 144, Mean Reward = -3.535, Threshold = -5.000\n",
      "Iteration = 145, Mean Reward = -0.155, Threshold = -6.000\n",
      "Iteration = 146, Mean Reward = -1.345, Threshold = -4.000\n",
      "Iteration = 147, Mean Reward = -1.380, Threshold = -7.000\n",
      "Iteration = 148, Mean Reward = -0.095, Threshold = -6.000\n",
      "Iteration = 149, Mean Reward = -0.070, Threshold = -4.000\n",
      "Iteration = 150, Mean Reward = -0.680, Threshold = -4.400\n",
      "Iteration = 151, Mean Reward = -0.575, Threshold = -6.000\n",
      "Iteration = 152, Mean Reward = 0.460, Threshold = -4.000\n",
      "Iteration = 153, Mean Reward = 1.755, Threshold = -3.200\n",
      "Iteration = 154, Mean Reward = -0.300, Threshold = -6.200\n",
      "Iteration = 155, Mean Reward = -0.845, Threshold = -7.000\n",
      "Iteration = 156, Mean Reward = 0.890, Threshold = -5.200\n",
      "Iteration = 157, Mean Reward = 0.945, Threshold = -4.000\n",
      "Iteration = 158, Mean Reward = 0.370, Threshold = -7.200\n",
      "Iteration = 159, Mean Reward = 2.115, Threshold = -4.000\n",
      "Iteration = 160, Mean Reward = -2.600, Threshold = -6.000\n",
      "Iteration = 161, Mean Reward = -0.020, Threshold = -6.000\n",
      "Iteration = 162, Mean Reward = 1.955, Threshold = -5.000\n",
      "Iteration = 163, Mean Reward = -0.440, Threshold = -5.000\n",
      "Iteration = 164, Mean Reward = -1.630, Threshold = -6.000\n",
      "Iteration = 165, Mean Reward = 0.485, Threshold = -5.200\n",
      "Iteration = 166, Mean Reward = -0.630, Threshold = -5.000\n",
      "Iteration = 167, Mean Reward = -1.225, Threshold = -6.200\n",
      "Iteration = 168, Mean Reward = -0.300, Threshold = -6.000\n",
      "Iteration = 169, Mean Reward = 0.645, Threshold = -5.000\n",
      "Iteration = 170, Mean Reward = -0.130, Threshold = -4.000\n",
      "Iteration = 171, Mean Reward = 0.935, Threshold = -4.000\n",
      "Iteration = 172, Mean Reward = 0.225, Threshold = -5.000\n",
      "Iteration = 173, Mean Reward = 0.230, Threshold = -5.000\n",
      "Iteration = 174, Mean Reward = 0.965, Threshold = -4.000\n",
      "Iteration = 175, Mean Reward = -0.030, Threshold = -5.000\n",
      "Iteration = 176, Mean Reward = 2.145, Threshold = -3.000\n",
      "Iteration = 177, Mean Reward = 1.170, Threshold = -5.000\n",
      "Iteration = 178, Mean Reward = -0.330, Threshold = -4.000\n",
      "Iteration = 179, Mean Reward = -1.165, Threshold = -6.000\n",
      "Iteration = 180, Mean Reward = 1.255, Threshold = -4.000\n",
      "Iteration = 181, Mean Reward = -1.570, Threshold = -6.000\n",
      "Iteration = 182, Mean Reward = -2.175, Threshold = -5.200\n",
      "Iteration = 183, Mean Reward = -2.980, Threshold = -9.000\n",
      "Iteration = 184, Mean Reward = -0.645, Threshold = -5.000\n",
      "Iteration = 185, Mean Reward = 0.650, Threshold = -5.000\n",
      "Iteration = 186, Mean Reward = -1.110, Threshold = -5.200\n",
      "Iteration = 187, Mean Reward = -2.675, Threshold = -4.200\n",
      "Iteration = 188, Mean Reward = -1.570, Threshold = -6.000\n",
      "Iteration = 189, Mean Reward = 0.225, Threshold = -4.000\n",
      "Iteration = 190, Mean Reward = -1.505, Threshold = -5.000\n",
      "Iteration = 191, Mean Reward = -2.110, Threshold = -7.000\n",
      "Iteration = 192, Mean Reward = 1.045, Threshold = -3.000\n",
      "Iteration = 193, Mean Reward = 1.475, Threshold = -4.000\n",
      "Iteration = 194, Mean Reward = 1.675, Threshold = -4.000\n",
      "Iteration = 195, Mean Reward = 2.755, Threshold = 0.000\n",
      "Iteration = 196, Mean Reward = 2.245, Threshold = -3.000\n",
      "Iteration = 197, Mean Reward = 0.825, Threshold = -5.000\n",
      "Iteration = 198, Mean Reward = 0.215, Threshold = -4.000\n",
      "Iteration = 199, Mean Reward = 3.230, Threshold = -2.200\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Create environment 'Taxi-v2'\n",
    "    env = gym.make('Taxi-v2')\n",
    "    env.reset()\n",
    "    env.render()\n",
    "\n",
    "    # Compute number of states for this environment\n",
    "    n_states = env.observation_space.n\n",
    "    # Compute number of actions for this environment\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    print('States number = %i, Actions number = %i' % (n_states, n_actions))\n",
    "\n",
    "    # Initialize policy - let's say random uniform\n",
    "    policy = np.full((n_states, n_actions), 1.0 / n_actions)\n",
    "    check_policy_init(policy)\n",
    "\n",
    "    # Complete generate session function\n",
    "    check_generate_session_func(generate_session)\n",
    "\n",
    "    # Complete select elites function\n",
    "    check_select_elites_func(select_elites)\n",
    "\n",
    "    # Complete update policy function\n",
    "    check_update_policy_func(update_policy)\n",
    "\n",
    "    # Complete rl_cross_entropy()\n",
    "    rl_cross_entropy()\n",
    "\n",
    "    # Close environment when everything is done\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
