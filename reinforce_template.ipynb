{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asergeev/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: predict_proba() function: OK!\n",
      "Test: generate_session() function: OK!\n",
      "Test: get_cumulative_rewards() function: OK!\n",
      "Iteration: 0, Mean reward:20.780\n",
      "Iteration: 1, Mean reward:32.380\n",
      "Iteration: 2, Mean reward:44.740\n",
      "Iteration: 3, Mean reward:47.130\n",
      "Iteration: 4, Mean reward:71.130\n",
      "Iteration: 5, Mean reward:96.320\n",
      "Iteration: 6, Mean reward:116.340\n",
      "Iteration: 7, Mean reward:154.020\n",
      "Iteration: 8, Mean reward:227.380\n",
      "Iteration: 9, Mean reward:304.160\n",
      "Iteration: 10, Mean reward:389.130\n",
      "Iteration: 11, Mean reward:448.510\n",
      "Iteration: 12, Mean reward:591.150\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_predict_proba():\n",
    "    test_states = np.array([env.reset() for _ in range(5)])\n",
    "    test_probas = predict_proba(test_states)\n",
    "    assert isinstance(test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "    assert tuple(test_probas.shape) == (test_states.shape[0], n_actions), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "    assert np.allclose(np.sum(test_probas, axis = 1), 1), \"probabilities do not sum to 1\"\n",
    "    print('Test: predict_proba() function: OK!')\n",
    "\n",
    "\n",
    "def test_generate_session():\n",
    "    states, actions, rewards = generate_session()\n",
    "    assert len(states) == len(actions) == len(rewards), \"length must be equal\"\n",
    "    print('Test: generate_session() function: OK!')\n",
    "\n",
    "\n",
    "def test_get_cumulative_rewards():\n",
    "    assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "    assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "    assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "    assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "    print('Test: get_cumulative_rewards() function: OK!')\n",
    "\n",
    "\n",
    "# < YOUR CODE HERE >\n",
    "# Build a simple neural network that predicts policy logits.\n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "class ReinforceAgent(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super(ReinforceAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim[0], 42)\n",
    "        self.fc2 = nn.Linear(42, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# < YOUR CODE HERE >\n",
    "def predict_proba(states):\n",
    "    \"\"\"\n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    some = F.softmax(agent(torch.FloatTensor(states)))\n",
    "\n",
    "    return some.data.numpy()\n",
    "\n",
    "\n",
    "# < YOUR CODE HERE >\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"\n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probas = predict_proba(np.array([s]))[0]\n",
    "        a = np.random.choice(n_actions, p=action_probas)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        # record session history to train later\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards\n",
    "\n",
    "\n",
    "# < YOUR CODE HERE >\n",
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                              ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session\n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    cumulative_rewards = [rewards.pop()]\n",
    "    for rew in reversed(rewards):\n",
    "        cumulative_rewards.append(rew + gamma * cumulative_rewards[-1])\n",
    "\n",
    "    return list(reversed(cumulative_rewards))\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot\n",
    "\n",
    "\n",
    "# < YOUR CODE HERE >\n",
    "def train_on_session(optimizer, states, actions, rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # cast everything into a variable\n",
    "    states = Variable(torch.FloatTensor(states))\n",
    "    actions = Variable(torch.IntTensor(actions))\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = Variable(torch.FloatTensor(cumulative_returns))\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = agent(states)\n",
    "    probas = F.softmax(logits, dim=1)\n",
    "    logprobas = F.log_softmax(logits, dim=1)\n",
    "\n",
    "    assert all(isinstance(v, Variable) for v in [logits, probas, logprobas]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_proba function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    logprobas_for_actions = torch.sum(logprobas * to_one_hot(actions), dim=1)\n",
    "\n",
    "    # REINFORCE objective function\n",
    "    J_hat = torch.mean(torch.dot(logprobas_for_actions, cumulative_returns))\n",
    "\n",
    "\n",
    "    loss = -1 * J_hat\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make(\"CartPole-v0\").env\n",
    "\n",
    "    env.reset()\n",
    "    n_actions = env.action_space.n\n",
    "    state_dim = env.observation_space.shape\n",
    "\n",
    "    env.render(\"rgb_array\")\n",
    "    env.close()\n",
    "\n",
    "    # 1. Complete ReinforceAgent class\n",
    "    # 2. Complete predict_proba()\n",
    "    # 3. Complete generate_session()\n",
    "    # 4. Complete get_cumulative_rewards()\n",
    "    # 5. Complete train_on_sessions()\n",
    "\n",
    "    # Create agent\n",
    "    agent = ReinforceAgent(state_dim, n_actions)\n",
    "    test_predict_proba()\n",
    "    test_generate_session()\n",
    "\n",
    "    test_get_cumulative_rewards()\n",
    "\n",
    "    # call train_on_sessions()\n",
    "    for i in range(100):\n",
    "        optimizer = optim.Adam(agent.parameters())\n",
    "        rewards = [train_on_session(optimizer, *generate_session()) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "        print(\"Iteration: %i, Mean reward:%.3f\" % (i, np.mean(rewards)))\n",
    "\n",
    "        if np.mean(rewards) > 500:\n",
    "            print(\"You Win!\")  # but you can train even further\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
