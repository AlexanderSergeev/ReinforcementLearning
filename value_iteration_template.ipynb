{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state = s0\n",
      "Next_state = s2, reward = 0.0, done = False\n",
      "mdp.get_all_states =  ('s0', 's1', 's2')\n",
      "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
      "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "mdp.get_reward('s1', 'a0', 's0') =  5\n",
      "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n",
      "Graphviz available:  True\n",
      "get_action_value() function: Ok!\n",
      "get_new_state_value() function: Ok!\n",
      "Iteration =    0 | Difference = 3.500 |   V(s0) = 0.000   V(s1) = 0.000   V(s2) = 0.000\n",
      "Iteration =    1 | Difference = 1.890 |   V(s0) = 0.000   V(s1) = 3.500   V(s2) = 0.000\n",
      "Iteration =    2 | Difference = 1.701 |   V(s0) = 0.000   V(s1) = 3.815   V(s2) = 1.890\n",
      "Iteration =    3 | Difference = 1.135 |   V(s0) = 1.701   V(s1) = 4.184   V(s2) = 2.060\n",
      "Iteration =    4 | Difference = 0.730 |   V(s0) = 1.854   V(s1) = 5.319   V(s2) = 2.871\n",
      "Iteration =    5 | Difference = 0.611 |   V(s0) = 2.584   V(s1) = 5.664   V(s2) = 3.540\n",
      "Iteration =    6 | Difference = 0.547 |   V(s0) = 3.186   V(s1) = 6.275   V(s2) = 3.989\n",
      "Iteration =    7 | Difference = 0.492 |   V(s0) = 3.590   V(s1) = 6.790   V(s2) = 4.535\n",
      "Iteration =    8 | Difference = 0.422 |   V(s0) = 4.082   V(s1) = 7.189   V(s2) = 4.959\n",
      "Iteration =    9 | Difference = 0.365 |   V(s0) = 4.463   V(s1) = 7.611   V(s2) = 5.352\n",
      "Iteration =   10 | Difference = 0.329 |   V(s0) = 4.816   V(s1) = 7.960   V(s2) = 5.717\n",
      "Iteration =   11 | Difference = 0.293 |   V(s0) = 5.145   V(s1) = 8.280   V(s2) = 6.032\n",
      "Iteration =   12 | Difference = 0.262 |   V(s0) = 5.429   V(s1) = 8.572   V(s2) = 6.323\n",
      "Iteration =   13 | Difference = 0.235 |   V(s0) = 5.691   V(s1) = 8.830   V(s2) = 6.584\n",
      "Iteration =   14 | Difference = 0.211 |   V(s0) = 5.925   V(s1) = 9.065   V(s2) = 6.817\n",
      "Iteration =   15 | Difference = 0.190 |   V(s0) = 6.135   V(s1) = 9.276   V(s2) = 7.028\n",
      "Iteration =   16 | Difference = 0.171 |   V(s0) = 6.325   V(s1) = 9.465   V(s2) = 7.218\n",
      "Iteration =   17 | Difference = 0.154 |   V(s0) = 6.496   V(s1) = 9.636   V(s2) = 7.388\n",
      "Iteration =   18 | Difference = 0.138 |   V(s0) = 6.649   V(s1) = 9.790   V(s2) = 7.542\n",
      "Iteration =   19 | Difference = 0.124 |   V(s0) = 6.788   V(s1) = 9.928   V(s2) = 7.680\n",
      "Iteration =   20 | Difference = 0.112 |   V(s0) = 6.912   V(s1) = 10.052   V(s2) = 7.805\n",
      "Iteration =   21 | Difference = 0.101 |   V(s0) = 7.024   V(s1) = 10.164   V(s2) = 7.917\n",
      "Iteration =   22 | Difference = 0.091 |   V(s0) = 7.125   V(s1) = 10.265   V(s2) = 8.017\n",
      "Iteration =   23 | Difference = 0.082 |   V(s0) = 7.216   V(s1) = 10.356   V(s2) = 8.108\n",
      "Iteration =   24 | Difference = 0.073 |   V(s0) = 7.297   V(s1) = 10.437   V(s2) = 8.190\n",
      "Iteration =   25 | Difference = 0.066 |   V(s0) = 7.371   V(s1) = 10.511   V(s2) = 8.263\n",
      "Iteration =   26 | Difference = 0.060 |   V(s0) = 7.437   V(s1) = 10.577   V(s2) = 8.329\n",
      "Iteration =   27 | Difference = 0.054 |   V(s0) = 7.496   V(s1) = 10.636   V(s2) = 8.389\n",
      "Iteration =   28 | Difference = 0.048 |   V(s0) = 7.550   V(s1) = 10.690   V(s2) = 8.442\n",
      "Iteration =   29 | Difference = 0.043 |   V(s0) = 7.598   V(s1) = 10.738   V(s2) = 8.491\n",
      "Iteration =   30 | Difference = 0.039 |   V(s0) = 7.641   V(s1) = 10.782   V(s2) = 8.534\n",
      "Iteration =   31 | Difference = 0.035 |   V(s0) = 7.681   V(s1) = 10.821   V(s2) = 8.573\n",
      "Iteration =   32 | Difference = 0.032 |   V(s0) = 7.716   V(s1) = 10.856   V(s2) = 8.608\n",
      "Iteration =   33 | Difference = 0.028 |   V(s0) = 7.747   V(s1) = 10.887   V(s2) = 8.640\n",
      "Iteration =   34 | Difference = 0.026 |   V(s0) = 7.776   V(s1) = 10.916   V(s2) = 8.668\n",
      "Iteration =   35 | Difference = 0.023 |   V(s0) = 7.801   V(s1) = 10.941   V(s2) = 8.694\n",
      "Iteration =   36 | Difference = 0.021 |   V(s0) = 7.824   V(s1) = 10.964   V(s2) = 8.717\n",
      "Iteration =   37 | Difference = 0.019 |   V(s0) = 7.845   V(s1) = 10.985   V(s2) = 8.738\n",
      "Iteration =   38 | Difference = 0.017 |   V(s0) = 7.864   V(s1) = 11.004   V(s2) = 8.756\n",
      "Iteration =   39 | Difference = 0.015 |   V(s0) = 7.881   V(s1) = 11.021   V(s2) = 8.773\n",
      "Iteration =   40 | Difference = 0.014 |   V(s0) = 7.896   V(s1) = 11.036   V(s2) = 8.788\n",
      "Iteration =   41 | Difference = 0.012 |   V(s0) = 7.909   V(s1) = 11.049   V(s2) = 8.802\n",
      "Iteration =   42 | Difference = 0.011 |   V(s0) = 7.922   V(s1) = 11.062   V(s2) = 8.814\n",
      "Iteration =   43 | Difference = 0.010 |   V(s0) = 7.933   V(s1) = 11.073   V(s2) = 8.825\n",
      "Iteration =   44 | Difference = 0.009 |   V(s0) = 7.943   V(s1) = 11.083   V(s2) = 8.835\n",
      "Iteration =   45 | Difference = 0.008 |   V(s0) = 7.952   V(s1) = 11.092   V(s2) = 8.844\n",
      "Iteration =   46 | Difference = 0.007 |   V(s0) = 7.960   V(s1) = 11.100   V(s2) = 8.852\n",
      "Iteration =   47 | Difference = 0.007 |   V(s0) = 7.967   V(s1) = 11.107   V(s2) = 8.859\n",
      "Iteration =   48 | Difference = 0.006 |   V(s0) = 7.973   V(s1) = 11.113   V(s2) = 8.866\n",
      "Iteration =   49 | Difference = 0.005 |   V(s0) = 7.979   V(s1) = 11.119   V(s2) = 8.872\n",
      "Iteration =   50 | Difference = 0.005 |   V(s0) = 7.984   V(s1) = 11.125   V(s2) = 8.877\n",
      "Iteration =   51 | Difference = 0.004 |   V(s0) = 7.989   V(s1) = 11.129   V(s2) = 8.882\n",
      "Iteration =   52 | Difference = 0.004 |   V(s0) = 7.993   V(s1) = 11.134   V(s2) = 8.886\n",
      "Iteration =   53 | Difference = 0.003 |   V(s0) = 7.997   V(s1) = 11.137   V(s2) = 8.890\n",
      "Iteration =   54 | Difference = 0.003 |   V(s0) = 8.001   V(s1) = 11.141   V(s2) = 8.893\n",
      "Iteration =   55 | Difference = 0.003 |   V(s0) = 8.004   V(s1) = 11.144   V(s2) = 8.896\n",
      "Iteration =   56 | Difference = 0.003 |   V(s0) = 8.007   V(s1) = 11.147   V(s2) = 8.899\n",
      "Iteration =   57 | Difference = 0.002 |   V(s0) = 8.009   V(s1) = 11.149   V(s2) = 8.902\n",
      "Iteration =   58 | Difference = 0.002 |   V(s0) = 8.011   V(s1) = 11.152   V(s2) = 8.904\n",
      "Iteration =   59 | Difference = 0.002 |   V(s0) = 8.014   V(s1) = 11.154   V(s2) = 8.906\n",
      "Iteration =   60 | Difference = 0.002 |   V(s0) = 8.015   V(s1) = 11.155   V(s2) = 8.908\n",
      "Iteration =   61 | Difference = 0.001 |   V(s0) = 8.017   V(s1) = 11.157   V(s2) = 8.909\n",
      "Iteration =   62 | Difference = 0.001 |   V(s0) = 8.019   V(s1) = 11.159   V(s2) = 8.911\n",
      "Iteration =   63 | Difference = 0.001 |   V(s0) = 8.020   V(s1) = 11.160   V(s2) = 8.912\n",
      "Iteration =   64 | Difference = 0.001 |   V(s0) = 8.021   V(s1) = 11.161   V(s2) = 8.913\n",
      "Iteration =   65 | Difference = 0.001 |   V(s0) = 8.022   V(s1) = 11.162   V(s2) = 8.915\n",
      "Done!\n",
      "Final state values: {'s0': 8.023123818663871, 's1': 11.163174814980803, 's2': 8.915559364985523}\n",
      "Checking final state_values: Ok!\n",
      "get_optimal_action() function : Ok!\n",
      "['a1', 'a0', 'a0']\n",
      "Average reward:  0.9295\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + 'D:/Anaconda/Library/bin/graphviz/'\n",
    "\n",
    "from mdp import MDP\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def check_generate_session_func(mdp, get_action_value_func):\n",
    "    test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "    assert np.allclose(get_action_value_func(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "    assert np.allclose(get_action_value_func(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)\n",
    "\n",
    "    print('get_action_value() function: Ok!')\n",
    "\n",
    "\n",
    "def check_get_new_state_value_func(mdp, get_new_state_value_func):\n",
    "    test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "    test_Vs_copy = dict(test_Vs)\n",
    "    assert np.allclose(get_new_state_value_func(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "    assert np.allclose(get_new_state_value_func(mdp, test_Vs, 's2', 0.9), 0.69)\n",
    "    assert test_Vs == test_Vs_copy, 'please do not change state_values in get_new_state_value'\n",
    "\n",
    "    print('get_new_state_value() function: Ok!')\n",
    "\n",
    "\n",
    "def check_state_values(state_values):\n",
    "    assert abs(state_values['s0'] - 8.032) < 0.01\n",
    "    assert abs(state_values['s1'] - 11.169) < 0.01\n",
    "    assert abs(state_values['s2'] - 8.921) < 0.01\n",
    "\n",
    "    print('Checking final state_values: Ok!')\n",
    "\n",
    "\n",
    "def check_get_optimal_action(get_optimal_action_func, mdp, state_values, gamma):\n",
    "    assert get_optimal_action_func(mdp, state_values, 's0', gamma) == 'a1'\n",
    "    assert get_optimal_action_func(mdp, state_values, 's1', gamma) == 'a0'\n",
    "    assert get_optimal_action_func(mdp, state_values, 's2', gamma) == 'a0'\n",
    "\n",
    "    print('get_optimal_action() function : Ok!')\n",
    "\n",
    "\n",
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) \"\"\"\n",
    "    q = 0\n",
    "    for s, prob_s in mdp.get_next_states(state, action).items():\n",
    "        q += prob_s * (mdp.get_reward(state, action, s) + gamma * state_values[s])\n",
    "        \n",
    "    return q\n",
    "\n",
    "\n",
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) .Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "\n",
    "    q = [get_action_value(mdp, state_values, state, action, gamma) \n",
    "         for action in mdp.get_possible_actions(state)]\n",
    "    \n",
    "    return max(q)\n",
    "\n",
    "\n",
    "def rl_value_iteration(mdp, gamma, num_iter, min_difference, init_state_values):\n",
    "    # Initialize V(s)\n",
    "    state_values = init_state_values\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        # Compute new state values using the functions you defined above.\n",
    "        # It must be a dict {state : float V_new(state)}\n",
    "        new_state_values = {s: get_new_state_value(mdp, state_values, s, gamma) for s in mdp.get_all_states()}\n",
    "\n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "        print('Iteration = %4i | Difference = %.3f |   ' % (i, diff), end='')\n",
    "        print('   '.join('V(%s) = %.3f' % (s, v) for s, v in state_values.items()), end='\\n')\n",
    "\n",
    "        # Updating state_values\n",
    "        state_values = new_state_values\n",
    "\n",
    "        if diff < min_difference:\n",
    "            print('Done!')\n",
    "            return state_values, True\n",
    "\n",
    "    return state_values, False\n",
    "\n",
    "\n",
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "\n",
    "    q = {a: get_action_value(mdp, state_values, state, a, gamma)\n",
    "         for a in mdp.get_possible_actions(state)}\n",
    "    \n",
    "    return max(q, key=q.get)\n",
    "\n",
    "\n",
    "def test_optimal_strategy(mdp, state_values, gamma, max_steps):\n",
    "    \"\"\" Test optimal strategy, derived from state_values. \"\"\"\n",
    "    rewards_at_each_step = []\n",
    "    s = mdp.reset()\n",
    "    for _ in range(max_steps):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards_at_each_step.append(r)\n",
    "\n",
    "    return rewards_at_each_step\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    transition_probs = {\n",
    "        's0': {\n",
    "            'a0': {'s0': 0.5, 's2': 0.5},\n",
    "            'a1': {'s2': 1}\n",
    "        },\n",
    "        's1': {\n",
    "            'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "            'a1': {'s1': 0.95, 's2': 0.05}\n",
    "        },\n",
    "        's2': {\n",
    "            'a0': {'s0': 0.4, 's1': 0.6},\n",
    "            'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "        }\n",
    "    }\n",
    "    rewards = {\n",
    "        's1': {'a0': {'s0': +5}},\n",
    "        's2': {'a1': {'s0': -1}}\n",
    "    }\n",
    "\n",
    "    gamma = 0.9  # Discount factor for MDP\n",
    "\n",
    "    mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "    print('Initial state =', mdp.reset())\n",
    "    next_state, reward, done, info = mdp.step('a1')\n",
    "    print('Next_state = %s, reward = %s, done = %s' % (next_state, reward, done))\n",
    "\n",
    "    print('mdp.get_all_states = ', mdp.get_all_states())\n",
    "    print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "    print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "    print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "    print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))\n",
    "\n",
    "    visualize = True\n",
    "    from mdp import has_graphviz\n",
    "    print('Graphviz available: ', has_graphviz)\n",
    "\n",
    "    if has_graphviz and visualize:\n",
    "        from mdp import plot_graph, plot_graph_with_state_values, plot_graph_optimal_strategy_and_state_values\n",
    "        plot_graph(mdp).render()\n",
    "\n",
    "    # Complete get_action_value().\n",
    "    check_generate_session_func(mdp, get_action_value)\n",
    "\n",
    "    # Complete get_new_state_value()\n",
    "    check_get_new_state_value_func(mdp, get_new_state_value)\n",
    "\n",
    "    # Let's combine everything together\n",
    "\n",
    "    # Complete rl_value_iteration()\n",
    "\n",
    "    # Test rl_value_iteration()\n",
    "    num_iter = 100  # Maximum iterations, excluding initialization\n",
    "    min_difference = 0.001  # stop Value Iteration if new values are this close to old values (or closer)\n",
    "\n",
    "    init_values = {s:0 for s in mdp.get_all_states()}\n",
    "    state_values, _ = rl_value_iteration(mdp, gamma, num_iter, min_difference, init_values)\n",
    "\n",
    "    # Draw state_values after training.\n",
    "    if has_graphviz and visualize:\n",
    "        plot_graph_with_state_values(mdp, state_values).render(filename='MDP_with_states')\n",
    "\n",
    "    print('Final state values:', state_values)\n",
    "    check_state_values(state_values)\n",
    "\n",
    "    # Complete get_optimal_action function.\n",
    "    check_get_optimal_action(get_optimal_action, mdp, state_values, gamma)\n",
    "\n",
    "    # Visualize optimal strategy.\n",
    "    if has_graphviz and visualize:\n",
    "        plot_graph_optimal_strategy_and_state_values(mdp, state_values, get_action_value, gamma).render(\n",
    "            filename='MDP_with_opt_strategy')\n",
    "\n",
    "    print([get_optimal_action(mdp, state_values, s, gamma=0.9) for s in mdp.get_all_states()])\n",
    "    # Test optimal strategy.\n",
    "    rewards = test_optimal_strategy(mdp, state_values, gamma, 10000)\n",
    "    print('Average reward: ', np.mean(rewards))\n",
    "    assert (0.85 < np.mean(rewards) < 1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
