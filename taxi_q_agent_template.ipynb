{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "States number = 500, Actions number = 6\n",
      "Iteration 0, Average reward -2500.00, Epsilon 0.090\n",
      "Iteration 100, Average reward -288.00, Epsilon 0.081\n",
      "Iteration 200, Average reward -161.79, Epsilon 0.073\n",
      "Iteration 300, Average reward -108.69, Epsilon 0.066\n",
      "Iteration 400, Average reward -81.12, Epsilon 0.059\n",
      "Iteration 500, Average reward -64.08, Epsilon 0.053\n",
      "Iteration 600, Average reward -52.52, Epsilon 0.048\n",
      "Iteration 700, Average reward -44.21, Epsilon 0.043\n",
      "Iteration 800, Average reward -37.96, Epsilon 0.039\n",
      "Iteration 900, Average reward -33.02, Epsilon 0.035\n",
      "Iteration 1000, Average reward -29.02, Epsilon 0.031\n",
      "Iteration 1100, Average reward -25.78, Epsilon 0.028\n",
      "Iteration 1200, Average reward -23.03, Epsilon 0.025\n",
      "Iteration 1300, Average reward -20.73, Epsilon 0.023\n",
      "Iteration 1400, Average reward -18.81, Epsilon 0.021\n",
      "Iteration 1500, Average reward -17.07, Epsilon 0.019\n",
      "Iteration 1600, Average reward -15.51, Epsilon 0.017\n",
      "Iteration 1700, Average reward -14.13, Epsilon 0.015\n",
      "Iteration 1800, Average reward -12.90, Epsilon 0.014\n",
      "Iteration 1900, Average reward -11.83, Epsilon 0.012\n",
      "Iteration 2000, Average reward -10.88, Epsilon 0.011\n",
      "Iteration 2100, Average reward -10.01, Epsilon 0.010\n",
      "Iteration 2200, Average reward -9.23, Epsilon 0.009\n",
      "Iteration 2300, Average reward -8.50, Epsilon 0.008\n",
      "Iteration 2400, Average reward -7.81, Epsilon 0.007\n",
      "Iteration 2500, Average reward -7.16, Epsilon 0.006\n",
      "Iteration 2600, Average reward -6.57, Epsilon 0.006\n",
      "Iteration 2700, Average reward -6.02, Epsilon 0.005\n",
      "Iteration 2800, Average reward -5.53, Epsilon 0.005\n",
      "Iteration 2900, Average reward -5.07, Epsilon 0.004\n",
      "Iteration 3000, Average reward -4.65, Epsilon 0.004\n",
      "Iteration 3100, Average reward -4.25, Epsilon 0.003\n",
      "Iteration 3200, Average reward -3.88, Epsilon 0.003\n",
      "Iteration 3300, Average reward -3.53, Epsilon 0.003\n",
      "Iteration 3400, Average reward -3.18, Epsilon 0.003\n",
      "Iteration 3500, Average reward -2.86, Epsilon 0.002\n",
      "Iteration 3600, Average reward -2.55, Epsilon 0.002\n",
      "Iteration 3700, Average reward -2.26, Epsilon 0.002\n",
      "Iteration 3800, Average reward -2.00, Epsilon 0.002\n",
      "Iteration 3900, Average reward -1.75, Epsilon 0.001\n",
      "Iteration 4000, Average reward -1.51, Epsilon 0.001\n",
      "Iteration 4100, Average reward -1.29, Epsilon 0.001\n",
      "Iteration 4200, Average reward -1.07, Epsilon 0.001\n",
      "Iteration 4300, Average reward -0.86, Epsilon 0.001\n",
      "Iteration 4400, Average reward -0.65, Epsilon 0.001\n",
      "Iteration 4500, Average reward -0.45, Epsilon 0.001\n",
      "Iteration 4600, Average reward -0.27, Epsilon 0.001\n",
      "Iteration 4700, Average reward -0.09, Epsilon 0.001\n",
      "Iteration 4800, Average reward 0.08, Epsilon 0.001\n",
      "Iteration 4900, Average reward 0.25, Epsilon 0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym.spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from qlearning_template import QLearningAgent\n",
    "\n",
    "\n",
    "def play_and_train(env, agent, t_max=10 ** 4):\n",
    "    \"\"\" This function should\n",
    "    - run a full game (for t_max steps), actions given by agent\n",
    "    - train agent whenever possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(t_max):\n",
    "        a = agent.get_action(state)\n",
    "        new_state, reward, done, _ = env.step(a)\n",
    "\n",
    "        total_reward += reward\n",
    "        agent.update(state, a, new_state, reward)\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    max_iterations = 5000\n",
    "    visualize = False\n",
    "    # Create Taxi-v2 env\n",
    "    env = gym.make('Taxi-v2').env\n",
    "    env.reset()\n",
    "    env.render()\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    print('States number = %i, Actions number = %i' % (n_states, n_actions))\n",
    "\n",
    "    # create q learning agent with\n",
    "    # alpha=0.5\n",
    "    # get_legal_actions = lambda s: range(n_actions)\n",
    "    # epsilon=0.1\n",
    "    # discount=0.99\n",
    "\n",
    "    agent = QLearningAgent(alpha = 0.5, get_legal_actions=lambda s: range(n_actions), epsilon=0.1, discount=0.99)\n",
    "\n",
    "    plt.figure(figsize=[10, 4])\n",
    "    rewards = []\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(max_iterations):\n",
    "        # Play & train game\n",
    "        # Update rewards\n",
    "        rewards.append(play_and_train(env, agent))\n",
    "        # Decay agent epsilon\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            agent.epsilon *= 0.9\n",
    "            print('Iteration {}, Average reward {:.2f}, Epsilon {:.3f}'.format(i, np.mean(rewards), agent.epsilon))\n",
    "\n",
    "        if visualize:\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(rewards, color='r')\n",
    "            plt.xlabel('Iterations')\n",
    "            plt.ylabel('Total Reward')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(rewards, bins=20, range=[-700, +20], color='blue', label='Rewards distribution')\n",
    "            plt.xlabel('Reward')\n",
    "            plt.ylabel('p(Reward)')\n",
    "            plt.draw()\n",
    "            plt.pause(0.05)\n",
    "            plt.cla()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
