{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mdp import FrozenLakeEnv\n",
    "from mdp import has_graphviz\n",
    "\n",
    "\n",
    "\n",
    "def draw_policy(mdp, state_values, fig=None):\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w, h), cmap='gray', interpolation='none', clim=(0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h)-.5)\n",
    "    ax.set_yticks(np.arange(w)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1), 'right': (1,0), 'up': (-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y,x].item()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None:\n",
    "                continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y,u*.3, -v*.3, color='m', head_width=0.1, head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.draw()\n",
    "    plt.pause(2)\n",
    "    if fig is not None:\n",
    "        plt.cla()\n",
    "\n",
    "\n",
    "def visualize_step_by_step(mdp, gamma, max_iter_number, min_difference):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    state_values = {state: 0 for state in mdp.get_all_states()}\n",
    "    for i in range(max_iter_number):\n",
    "        new_state_values, done = rl_value_iteration(mdp, gamma, 1, min_difference, state_values)\n",
    "        if done:\n",
    "            break\n",
    "        draw_policy(mdp, new_state_values, fig)\n",
    "        state_values = new_state_values\n",
    "\n",
    "\n",
    "def mass_gaming(mdp, gamma, num_iter, games_number, steps_number):\n",
    "    state_values = {state: 0 for state in mdp.get_all_states()}\n",
    "    state_values, _ = rl_value_iteration(mdp, gamma, num_iter, min_difference, state_values)\n",
    "\n",
    "    total_rewards = []\n",
    "    for game_i in range(games_number):\n",
    "        s = mdp.reset()\n",
    "        rewards = []\n",
    "        for t in range(steps_number):\n",
    "            # s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "            rewards.append(r)\n",
    "            if done:\n",
    "                break\n",
    "        total_rewards.append(np.sum(rewards))\n",
    "    print('Average reward: ', np.mean(total_rewards))\n",
    "    if mdp.slip_chance == 0:\n",
    "        assert (1.0 <= np.mean(total_rewards) <= 1.0)\n",
    "    else:\n",
    "        assert (0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "    print('Well done!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    visualize = True\n",
    "    mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.1)\n",
    "    mdp.render()\n",
    "\n",
    "    gamma = 0.9\n",
    "    num_iter = 100\n",
    "    min_difference = 1e-5\n",
    "\n",
    "    # Play in Frozen Lake Env\n",
    "    state_values = {}  # Initialize state_values\n",
    "\n",
    "    # Run value iteration algo!\n",
    "    state_values, _ = None, None\n",
    "\n",
    "    # See how our agent performs - e.g. render what is going on when agent choose `optimal` value\n",
    "    s = mdp.reset()\n",
    "    mdp.render()\n",
    "    rewards = []  # Save all rewards to see mean reward.\n",
    "\n",
    "    # Your code here!\n",
    "\n",
    "    print('Average reward: ', np.mean(rewards))\n",
    "\n",
    "    # if visualize:\n",
    "    #     draw_policy(mdp, state_values)\n",
    "\n",
    "    # Let's see how it is improving in time.\n",
    "    # visualize_step_by_step(mdp, gamma, num_iter, min_difference)\n",
    "\n",
    "    # Express test!\n",
    "    mass_gaming(mdp, gamma, num_iter, 1000, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
