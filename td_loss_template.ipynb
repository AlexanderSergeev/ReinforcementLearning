{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import gym\n",
    "from dqn_agent import DQNAgent\n",
    "from preprocess import PreprocessAtari\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "\n",
    "def compute_td_loss(states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "    states = Variable(torch.FloatTensor(states))  # shape: [batch_size, c, h, w]\n",
    "    actions = Variable(torch.LongTensor(actions))  # shape: [batch_size]\n",
    "    rewards = Variable(torch.FloatTensor(rewards))  # shape: [batch_size]\n",
    "    next_states = Variable(torch.FloatTensor(next_states))  # shape: [batch_size, c, h, w]\n",
    "    is_done = Variable(torch.FloatTensor(is_done.astype('float32')))  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = None\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = None\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = None\n",
    "\n",
    "    next_state_values = next_state_values * is_not_done\n",
    "\n",
    "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[\n",
    "        0], \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    target_qvalues_for_actions = None\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim() == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim() == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim() == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make(\"BreakoutDeterministic-v0\")  # create raw env\n",
    "    env = PreprocessAtari(env)\n",
    "\n",
    "    observation_shape = env.observation_space.shape\n",
    "    n_actions = env.action_space.n\n",
    "    state_dim = observation_shape\n",
    "    env.reset()\n",
    "    obs, _, _, _ = env.step(env.action_space.sample())\n",
    "    agent = DQNAgent(state_dim, n_actions, epsilon=0.5)\n",
    "    target_network = DQNAgent(state_dim, n_actions)\n",
    "\n",
    "    exp_replay = ReplayBuffer(10)\n",
    "    for _ in range(30):\n",
    "        exp_replay.add(env.reset(), env.action_space.sample(), 1.0, env.reset(), done=False)\n",
    "\n",
    "    target_network.load_state_dict(agent.state_dict())\n",
    "    # sanity checks\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
    "\n",
    "    loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch, gamma=0.99,\n",
    "                           check_shapes=True)\n",
    "    loss.backward()\n",
    "\n",
    "    assert np.any(next(agent.parameters()).grad.data.numpy() != 0), \"loss must be differentiable w.r.t. network weights\"\n",
    "    print(\"TD Loss OK\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
